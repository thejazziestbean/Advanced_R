## title: "sentiment_analysis"
output: html_document
date: "2025-02-08"

the sentiments datasets

there are a variety of methods and dictionaries that exist for evaluating the opnion or emotion of the text.

AFFIN
bing
nrc

bing categories words in a binary fashion into positive or negative nrc categories into positive, negative, anger, anticipation, disgust, fear, jot, sadness, surprise, and trust.
AFFIN assigns a score between -5 and 5, with negative indicating negative sentiment, and 5 positive.

the function get_sentiments() allows us to get the specific sentiments lexicon with the measures for each one.

```

library(tidytext)
library(textdata)

afinn <- get_sentiments("afinn")

afinn

```

lets look at bing

```

bing <- get_sentiments("bing")

bing

```

and lastly nrc

```

nrc <- get_sentiments("nrc")

nrc

```

these libraries were created either using crowdourcing or cloud computing/ai like anmazon mechanical turk, or by labor of one of the authors, and then validated with crowdsourcing.

lets look at the words with a joy score from NRC

```
*   You should see the preprocessed data in the console.

```

Here's the complete code:

```
install.packages(tidy_books)

library(gutenbergr)
library(dplyr)
library(stringr)

library(gutenbergr)

darwin <- gutenberg_download(c(944, 1227, 1228, 2300), mirror = "<http://aleph.gutenberg.org>")

tidy_books <- darwin %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

tidy_books

```

lets add the book name instead of GTD

```

colnames (tidy_books)[1] <- "book"

tidy_books$book[tidy_books$book == 944] <- "The Voyage of the Beagle"
tidy_books$book[tidy_books$book == 1227] <- "The Expression of the Emotions in Man and Animals"
tidy_books$book[tidy_books$book == 1228] <- "On the Origin of Species by Means of Natural Selection"
tidy_books$book[tidy_books$book == 2300] <- "The Descent of Man, and Selection in Relation to Sex"

tidy_books

```

now that we have a tidy format with one per row, we are ready for sentiment analysis. first lets use NRC.

```

nrc_joy <- get_sentiments("nrc") %>%
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "The Voyage of the Beagle") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)

```

we can also examine how sentiment changes throughout a work.

```

library(tidyr)

colnames(tidy_books)

tidy_books <- tidy_books %>%
  mutate(lineNumber = row_number())

Charles_Darwin_Sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = lineNumber%/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(sentiment = positive - negative)

```

now lets plot it

```

library(ggplot2)

ggplot(Charles_Darwin_Sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")

```

lets compare the three sentiment dictions

there are several options for sentiment lexicons,you might want more info on which is appropriate for your purpose. here we will use all three our dictionaries and examine how the sentiment changes across the arc of TVOTB.

```

library(tidyr)

voyage <- tidy_books %>%
  filter(book == "The Voyage of the Beagle")

voyage

```

lets again use integer division ('%/%') to define larger sections of the text that span multiple lines, and we can use the same pattern with 'count()', 'pivot_wider()', and 'mutate()', to find the net sentiment of these sections of text.

```

affin <- voyage %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(index = lineNumber %/% 80) %>%
  summarise(sentiments = sum(value)) %>%
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  voyage %>%
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  voyage %>%
    inner_join(get_sentiments("nrc")%>%
                 filter(sentiment %in% c("positive", "negative"))
               ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = lineNumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>%
  mutate(sentiment = positive - negative)

```

we can now estimeate the net sentiment (positive - negative) in each chunk of the novel text for each lexion (dictionary).

lets bind them all together to visualize with ggplot

```

bind_rows(affin, bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")

```

let's look at the counts based on each dictionary

```

get_sentiments("nrc") %>%
  filter(sentiment %in% c("positive", "negative")) %>%
  count(sentiment)

```

```

get_sentiments("bing") %>%
  count(sentiment)

```

```

bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts

```

this can be shown visually, and we can pipe straight into ggplot2

```

bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scale = "free_y") +
  labs(x = "Contribution to Sentiment", y = NULL)

```

lets spot an anomoly in the dataset

```

custom_stop_words <- bind_rows(tibble(word = c("wild", "dark", "great", "like"), lexicon = c("custom")), stop_words)

custom_stop_words

```

Word Clouds!

We can see that tidy text mining and sentiment analysis works well with ggplot2, but having our data in tidy format leads to other nice graphing techniques

lets use the wordcloud package!!

```

library(wordcloud)

tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

```

we can change to matrix the acast() function.

```

library(reshape2)

tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"), max.words = 100)

```

looking at units beyond words

lots of useful work can be done by tokenizing the word level, but sometimes it's nice to look at different units of text. for example, we can look beyond just unigrams.

ex I am not having a good day

```
colnames(tidy_books)

tidy_books <- tidy_books %>%
  mutate(chapter = (lineNumber %/% 500) + 1)

```

```

bingnegative <- get_sentiments("bing") %>%
  filter(sentiment == "negative")

wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords / words) %>%
  filter(chapter != 0) %>%
  slice_max(ratio, n=1) %>%
  ungroup()

```
